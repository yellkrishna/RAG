# -*- coding: utf-8 -*-
"""L1-Advanced_RAG_Pipeline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A7cmHMPBF_vM9po92vi_kIWyNZbPtTgv

# Lesson 1: Advanced RAG Pipeline
"""

from dependencies import *

openai.api_key = os.environ["OPENAI_API_KEY"]

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))
# Suppress FutureWarning from sklearn
warnings.filterwarnings("ignore", category=FutureWarning)

input_list = []
for docs in os.listdir("Data\input"):
    input_list.append(f"Data\input\{docs}")

documents = SimpleDirectoryReader(
    input_files=input_list
).load_data()

print(type(documents), "\n")
print(len(documents), "\n")
print(type(documents[0]))
print(documents[0])
print(f" docs is a {type(documents)}, of length {len(documents)}, where each element is a {type(documents[0])} object")

"""Basic RAG pipeline"""

document = Document(text="\n\n".join([doc.text for doc in documents]))

"""An instance of the OpenAI class is created, which serves as an interface to OpenAI's language models.ServiceContext object is created using the from_defaults method. This method typically initializes a ServiceContext object with default settings. The ServiceContext provides a context for various services used in the indexing and querying process. This line creates a VectorStoreIndex object, which is responsible for indexing and organizing documents for efficient retrieval and querying. The method from_documents is used to create the index from a list of documents. The chunking, embedding and indexing is done in this one line."""

llm = OpenAI(model="gpt-3.5-turbo", temperature=0.1)
service_context = ServiceContext.from_defaults(
    llm=llm, embed_model="local:BAAI/bge-small-en-v1.5"
)
index = VectorStoreIndex.from_documents([document],
                                        service_context=service_context)

"""The query engine is created using the as_query_engine method of the index object. The response_mode parameter is set to "tree_summarize" and the vector_store_query_mode parameter is set to "svm". The query engine is then used to query the index for the explanation of the Kullback-Leibler Divergence. The response is printed to the console. The query engine is then used to query the index for the explanation of the difference between the Support Vector Machine and Linear Regression. The response is printed to the console."""

query_engine = index.as_query_engine(response_mode="tree_summarize", vector_store_query_mode="svm")

response = query_engine.query(
    "Explain the Kullback-Leibler Divergence?"
)
print(str(response))

response = query_engine.query(
    "Explain how the SVM differes from Linear Regression?"
)
print(str(response))